\documentclass{article}
\usepackage[a4paper, portrait, margin=1in]{geometry}

\usepackage{kotex, amsmath, amsfonts, amsthm, amssymb, bm}
\usepackage{setspace, indentfirst}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

%\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]

\newtheorem{definition}[theorem]{Definition}
%\newtheorem{problem}{Problem}
\newtheorem*{remark}{Remark}
\newtheorem{cor}[theorem]{Corollary}
%\newtheorem{cors}{Corollaries}
\newtheorem*{ex}{Example}

\let\oldref\ref
\renewcommand{\ref}[1]{(\oldref{#1})}

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\dom}{\text{dom}}
\DeclareMathOperator{\ran}{\text{ran}}

\title{CS 224n Assignment \#2: word2vec}
\author{\textbf{Intae Jun}}
\date{}

\begin{document}
\maketitle
\doublespacing

\section{Written: Understanding word2vec (31 points)}
\begin{enumerate}[label=(\alph*)]
    % (a) %
    \item (2 points) Prove that the naive-softmax loss (Eq. 2) is the same as the cross-entropy loss between $\bm{y}$ and $\hat{\bm{y}}$.\\
    \noindent Since the true label $\bm{y}_w$ is a one-hot vector, all the terms where $w \ne o$ are reduced and only the term where $w=o$ remains.

    % (b) %
    \item (7 points)
        \begin{enumerate}[label=(\roman*)]
            \item Compute the partial derivative of $\bm{J}_{\text{naive-softmax}}(\bm{v}_c, o, \bm{U})$ with respect to $\bm{v}_c$.\\
            \underline{Please write your answer in terms of $\bm{y}, \hat{\bm{y}}, \bm{U}$, and show your work toreceive full credit.}
            \begin{align*}
                \frac{\partial \bm{J}_{\text{naive-softmax}}(\bm{v}_c, o, \bm{U})}{\partial \bm{v}_c} &=
                -\frac{\partial \log P(O=o \vert C=c)}{\partial \bm{v}_c} \\
                &= -\frac{1}{P(O=o \vert C=c)} \frac{\partial P(O=o \vert C=c)}{\partial \bm{v}_c}\\
                &= -\frac{\bm{u}_o \exp(\bm{u}_o^T \bm{v}_c) \sum_{w\in\text{Vocab}}\exp(\bm{u}_w^T\bm{v}_c) - \exp(\bm{u}_o^T\bm{v}_c)\sum_{w \in \text{Vocab}} \bm{u}_w \exp(\bm{u}_w^T\bm{v}_c)}{P(O=o \vert C=c)(\sum_{w \in \text{Vocab}} \exp(\bm{u}_w^T\bm{v}_c))^2}\\
                &= -\frac{\bm{u}_o \exp(\bm{u}_o^T \bm{v}_c) \sum_{w\in\text{Vocab}}\exp(\bm{u}_w^T\bm{v}_c) - \exp(\bm{u}_o^T\bm{v}_c)\sum_{w \in \text{Vocab}} \bm{u}_w \exp(\bm{u}_w^T\bm{v}_c)}
                    {\exp(\bm{u}_o^T \bm{v}_c)(\sum_{w\in\text{Vocab}}\exp(\bm{u}_w^T\bm{v}_c))}\\
                &= - \frac{\bm{u}_o \sum_{w\in\text{Vocab}} \exp(\bm{u}_w^T\bm{v}_c) - \sum_{w\in\text{Vocab}} \bm{u}_w \exp(\bm{u}_w^T\bm{v}_c))}
                    {\sum_{w\in\text{Vocab}}\exp(\bm{u}_w^T\bm{v}_c)}\\
                &= -\bm{u}_o + \sum_{w\in\text{Vocab}} P(O=w \vert C=c) \bm{u}_w\\
                &= \bm{U}^T(\hat{\bm{y}} - \bm{y}).
            \end{align*}

            \item When $\hat{\bm{y}} - \bm{y}=0$, the gradient becomes zero.
            
            \item As the gradient is subtracted from the word vector $\bm{v}_c$, the vector is updated as follows:
            \begin{equation*}
                \bm{v}_c^{(i+1)} \leftarrow \bm{v}_c^{(i)} \eta \bm{U}^T(\hat{\bm{y}} - \bm{y}),
            \end{equation*} where $i$ is the index for the iteration number and eta is the learning rate for the gradient descent method.
            By subtracting this gradient, we are moving the center word vector $\bm{v}_c$ towards the correct output vector $\bm{u}_o$ and away from the incorrect predictoins $\bm{U}^T\hat{\bm{y}}$.
        
            \item For two different words $x\ne y$, if, for some scalar $\alpha$, they have a relationship such as $\bm{u}_x = \alpha \bm{u}_y$,
            the L2 normalized vectors $\bm{u}_x'$ and $\bm{u}_y'$ are not discriminated since L2 normalization makes every vector have the same magnitude.
            However the primary information contained in the vectors, such as representation of each word does not disappear.
            Also, keeping consistency in vector scale are recommended for some models that are sensitive to the scale of input features.
        \end{enumerate}
    
    \item (5 points) Compute the partial derivatives of $\bm{J}_{\text{naive-softmax}}(\bm{v}_c, o, \bm{U})$ with respect to each of the 'outside' word vectors, $\bm{u}_w$'s.
    There will be two cases: when $w=o$, the true 'outside' word vector, and $w\ne o$, for all other words.
    Please write your answer in terms of $\bm{y}, \hat{\bm{y}}$, and $\bm{v}_c$. In this subpart, you may use specific elements within these terms as well (such as $\bm{y}_1, \bm{y}_2, ...$).
    \begin{align*}
        \bm{J}_{\text{naive-softmax}}(\bm{v}_c, o, \bm{U}) = -\log P(O=o \vert C=c) = -\log \frac{\exp (\bm{u}_o^T \bm{v}_c)}{\sum_{w} \exp(\bm{u}_w^T \bm{v}_c)} = -\bm{u}_o^T \bm{v}_c + \log \sum_w \exp (\bm{u}_w^T \bm{v}_c).
    \end{align*}
    If $w=o$ (i.e. the outside word is the answer):
    \begin{align*}
        \frac{\partial \bm{J}}{\partial \bm{u}_w} &= -\bm{v}_c + \frac{\partial}{\partial \bm{u}_w} (\log \sum_w \exp (\bm{u}_w^T \bm{v}_c)) \\
        &= -\bm{v}_c + \frac{\bm{v}_c \sum_w \exp (\bm{u}_w^T \bm{v}_c)}{\log \sum_w \exp (\bm{u}_w^T \bm{v}_c)} \\
        &= -\bm{v}_c + P(O=o \vert C=c)\bm{v}_c\\
        &= (\hat{\bm{y}} - \bm{y})^T \bm{v}_c.
    \end{align*}
    If $w\ne o$ (i.e. the outside word is not the answer):
    \begin{align*}
        \frac{\partial \bm{J}}{\partial \bm{u}_w} &= -\frac{\partial}{\partial \bm{u}_w} (\bm{u}_o^T \bm{v}_c) + \frac{\partial}{\partial \bm{u}_w} (\log \sum_w \exp (\bm{u}_w^T \bm{v}_c)) \\
        &= 0 + \frac{\bm{v}_c \sum_w \exp (\bm{u}_w^T \bm{v}_c)}{\log \sum_w \exp (\bm{u}_w^T \bm{v}_c)} \\
        &= P(O=w \vert C=c) \bm{v}_c \\
        &= \hat{\bm{y}}^T \bm{v}_c \\ &= (\hat{\bm{y}} - \bm{y})^T \bm{v}_c.
    \end{align*}

    % (d) %
    \item (1 point) Write down the partial derivative of $\bm{J}_{\text{naive-softmax}}(\bm{v}_c, o, \bm{U})$ with respect to $\bm{U}$.
    \begin{align*}
        .
    \end{align*}

    % (e) %
    \item (2 points) Compute the derivate of $f(x)$ with respect to $x$.
    
    \noindent If $x> 0$, $f(x)=x$ and $f'(x)=1$. If $x<0$, then $f(x)=\alpha x$ and $f'(x)= \alpha$.\\
    In summary, we have that
    \begin{align*}
        \frac{d}{dx}f(x) = \begin{cases}
            x\ &(x>0),\\
            \alpha\ &(x<0).
        \end{cases}
    \end{align*}

    % (f) %
    \item (3 points) Compute the derivative of $\sigma(x)$ with respect to $x$, where $x$ is a scalar.
    \begin{align*}
        \frac{d}{dx} \left( \frac{e^x}{e^x +1} \right) &= \frac{e^x(e^x+1) - e^x(e^x)}{(e^x + 1)^2}\\
        &= \frac{e^x}{(e^x+1)^2} \\
        &= \sigma(x)(1-\sigma(x)).
    \end{align*}

    % (g) %
    \item (6 points)
    \begin{enumerate}[label=(\roman*)]
        \item .
    \end{enumerate}

    % (h) %
    \item (2 points)
    \begin{enumerate}[label=(\roman*)]
        \item .
    \end{enumerate}
\end{enumerate}


\end{document}